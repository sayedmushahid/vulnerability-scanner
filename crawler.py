import os
import requests
import re
from urllib.parse import urljoin
import sys


class webCrawler:

    def __init__(self):
        self.response = None

    def extract_links(self, url):
        try:
            print('[+] Extracting links ...\n')
            self.response = requests.get(url)
            linkList = re.findall('(?:href=")(.*?)"', self.response.content.decode('utf8'))
            print('[+] Link extraction successful !!\n')
            return linkList

        except Exception as e:
            print(f'[-] ERROR: {e}')
            sys.exit(0)


def print_links(href_link, target_url, path):
    try:
        test = []
        print('[+] Printing links...\n')
        for link in href_link:
            link = urljoin(target_url, link)
            if link.startswith(target_url):
                test.append(link)
                test = list(dict.fromkeys(test))
        with open(path + "/extracted_links.txt", "w") as outfile:
            outfile.write("\n".join(test))
            outfile.close()
        print(test)

    except Exception as e:
        print(f'[-] ERROR: {e}')
        sys.exit(0)